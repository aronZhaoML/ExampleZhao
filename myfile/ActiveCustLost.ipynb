{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext \n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext(\"local\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql1 = \"select * from active_cust_info_table\"\n",
    "df = spark.sql(sql1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看各个特征变量空值占比\n",
    "import pyspark.sql.functions as fn\n",
    "df.agg(*[(1-(fn.count(c)/fn.count(\"*\"))).alias(c+\"_missing\") for c in df.columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#空值处理\n",
    "df = df.withColumn(\"Feature1\",fn.when(df[\"fearture1\"].isNull()==True,fn.lit('99')).otherwise(df[\"Feature1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看所有特征\n",
    "for f in df.dtypes:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征分类 字符型 数值型\n",
    "cFlist = []\n",
    "dFlist = []\n",
    "for f in df.dtypes:\n",
    "    if f[1] == \"string\":\n",
    "        cFlist.append(f[0])\n",
    "    else:\n",
    "        dFlist.append(f[1])\n",
    "prrint(len(cFlist), len(dFlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#字符型特征 分类 唯一值 二值 多值\n",
    "uniqueFlist = []\n",
    "binaryFlist = []\n",
    "moreVFlist = []\n",
    "for i in range(len(cFlist)):\n",
    "    l = df.select(df[cFlist[i]]).distinct().count()\n",
    "    if l==1:\n",
    "        uniqueFlist.append(cFlist[i])\n",
    "    elif l==2:\n",
    "        binaryFlist.append(cFlist[i])\n",
    "    else:\n",
    "        moreVFlist.append(cFlist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(uniqueFlist),uniqueFlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(binaryFlist),binaryFlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(moreVFlist),moreVFlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数值型变量转化化double，二值型字符变量如果为（1，0）也转化为double\n",
    "import pyspark.sql.types\n",
    "df = df.select(cFlist + [fn.col(column).cast(\"double\").alias(column) for column in dFlist])\n",
    "for f in binaryFlist:\n",
    "    df = df.withColumn(f,fn.col(f).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征分类 字符型 数值型\n",
    "cFlist = []\n",
    "dFlist = []\n",
    "for f in df.dtypes:\n",
    "    if f[1] == \"string\":\n",
    "        cFlist.append(f[0])\n",
    "    else:\n",
    "        dFlist.append(f[1])\n",
    "prrint(len(cFlist), len(dFlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看数值型 唯一值变量\n",
    "for i in range(len(dFlist)):\n",
    "    l = df.select(df[dFlist[i]]).distinct().count()\n",
    "    if l==1:\n",
    "        uniqueFlist.append(dFlist[i])\n",
    "print(len(uniqueFlist),uniqueFlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#剔除唯一值变量\n",
    "for f in uniqueFlist:\n",
    "    if f in dFlist:\n",
    "        dFlist.remove(f)\n",
    "print(len(dFlist),dFlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#变量相关性分析\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df[dFlist]\n",
    "assembler = VectorAssembler(inputCols=dFlist,outputCol=\"features\")\n",
    "df_d = assembler.transform(df_d)\n",
    "df_d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"max_columns\",500)\n",
    "pd.set_option(\"max_rows\",1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = Correlation.corr(df_d,\"features\").head()\n",
    "r_pd = pd.DataFrame(r1[0].toArray(),index=dFlist,columns=dFlist)\n",
    "r_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pd[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.subplots(figsize = (16,5))\n",
    "sns.heatmap(r_pd[r_pd.abs()>0.5],annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#剔除强相关变量\n",
    "str_corr_feature = []\n",
    "row = r_pd.shape[0]\n",
    "for i in range(row):\n",
    "    for j in range(i+1,row):\n",
    "        if r_pd.iloc[i,j] >= 0.8 or r_pd.iloc[i,j] <= -0.8:\n",
    "            str_corr_feature.append(r_pd.columns[j])\n",
    "            \n",
    "for n in set(str_corr_feature):\n",
    "    dFlist.remove(n)\n",
    "print(dFlist,len(dFlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对多值字符型变量 卡方检验\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder,VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df.select([\"label\"] + moreVFlist)\n",
    "IndexFlist = []\n",
    "for f in moveVFlist:\n",
    "    categoryIndexer = StringIndexer(inputCol = f, outputCol = f+\"_Index\")\n",
    "    categoryTransformer = categoryIndexer.fit(df_c)\n",
    "    df_c = categoryTransformer.transform(df_c)\n",
    "    IndexFlist.append(f+\"_Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = IndexFlist, outputCol = \"features\")\n",
    "df_c = assembler.transform(df_c)\n",
    "df_c.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "ChiSqResult = ChiSquareTest.test(df_c,\"features\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChiSqResult.select(\"pValues\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFeature=dFlist + moreVFlist\n",
    "print(len(modelFeature),modelFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y由于抽样 回导致 数值型变量 一些频数较少的值在训练集中缺失 导致预测时保存 建议处理训练集，使包括多值字符型的所有值\n",
    "df_m = df.select(modelFeature)\n",
    "train_df,test_df = df_m.randomSplit([0.7,0.3])\n",
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(\"label\").agg({\"id\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建模\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "MFeatures = []\n",
    "tempIndex = []\n",
    "for f in moreVFlist:\n",
    "    stages.append(StringIndexer(inputCol=f,outputCol=f+\"_Index\"))\n",
    "    tempIndex.append(f+\"_Index\")\n",
    "for i in range(len(tempIndex)):\n",
    "    stages.append(OneHotEncoder(droplast=False,inputCol=tempIndex[i],outputCol=moreVFlist[i] + \"_vector\"))\n",
    "    MFeatures.append(moreVFlist[i] + \"_vector\")\n",
    "stages.append(VectorAssembler(inputCols=dFlist+MFeatures,outputCol=\"features\"))\n",
    "stages.append(RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",numTrees=200,maxBins=20,subsamplingRate=0.8,seed=2021))\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\",metricName = \"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predicted)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取输入模型vector的features的各个分量的名称 dFlist中的第一个值是label\n",
    "MoreVFeature_vector = []\n",
    "for f in MFeature:\n",
    "    l = len(predicted.select(f).head(1)[0][f].toArray().tolist())\n",
    "    for i in range(l):\n",
    "        MoreVFeature_vector.append(f+str(i+1))\n",
    "Features_name =dFlist[1:] + MoreVFeature_vector        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征变量重要性\n",
    "fimp = pipelineModel.stages[-1].featureImportances\n",
    "Fimp = pd.DataFrame(list(zip(Features_name,fimp.toArray().tolist())),columns = [\"Feature\",\"Importances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fimp.sort_values(\"Importances\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = predicted.filter(predicted[\"prediction\"]==1).filter(predicted[\"label\"]==1).count()\n",
    "FN = predicted.filter(predicted[\"prediction\"]==0).filter(predicted[\"label\"]==1).count()\n",
    "TN = predicted.filter(predicted[\"prediction\"]==0).filter(predicted[\"label\"]==0).count()\n",
    "FP = predicted.filter(predicted[\"prediction\"]==1).filter(predicted[\"label\"]==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = 1  FN = 2\n",
      "FP = 1  TN = 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TP = %d  FN = %d\\nFP = %d  TN = %d\\n\"%(TP,FN,FP,TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "命中率： 0.5\n",
      "召回率： 0.3333333333333333\n",
      "F1: 0.4\n"
     ]
    }
   ],
   "source": [
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F1 = 2*precision*recall/(precision+recall)\n",
    "print(\"命中率：\",precision)\n",
    "print(\"召回率：\",recall)\n",
    "print(\"F1:\",F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    return (row.label,) + tuple(row.probability.toArray().tolist())\n",
    "\n",
    "rd=predicted.select(\"label\",\"probability\").rdd.map(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rd.toDF()\n",
    "r = r.select(fn.col(\"_1\").alias(\"label\"), fn.round(\"_2\",2).alias(\"np\"),fn.round(\"_3\",2).alias(\"pp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.groupby(\"pp\").agg({\"np\":\"count\",\"lable\":\"sum\"}).show(1000,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "pipelineModel.save(\"./ActiveCustLostModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
